{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC-LZ20S_WUr"
   },
   "source": [
    "# Assignment 2: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9xqCFJBv_WUt"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import ne_chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZNmDWxj-V-J"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Use file `football_players.txt` to perform various information extraction tasks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V9YE4n6u7olU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 24172  100 24172    0     0  17118      0  0:00:01  0:00:01 --:--:-- 17118\n"
     ]
    }
   ],
   "source": [
    "# Download the text file (uncomment the line below in this cell, if not already downloaded from Blackboard)\n",
    "!curl \"https://ideone.com/plain/OvwDXZ\" > football_players.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpSaij2b73Vj"
   },
   "source": [
    " Read all the documents from `football_players.txt` into a list called `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qteh89cs7q4x"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "with open(\"football_players.txt\", \"r\",encoding=\"utf-8\") as G:\n",
    "    for text in G:\n",
    "        w = text.split('\\n')\n",
    "        docs.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCEJrJ-p_WU1"
   },
   "source": [
    "## Task 2 \n",
    "Function that takes a document and returns a list of sentences with part-of-speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "  # your code goes here\n",
    "    sent_list = []\n",
    "    pos_list = []\n",
    "    tagged_sent = []\n",
    "    \n",
    "    for sent in document:\n",
    "        sent_list += sent_tokenize(sent)\n",
    "\n",
    "    for line in sent_list:\n",
    "        words = word_tokenize(line)\n",
    "        pos_list = nltk.pos_tag(words)\n",
    "        tagged_sent.append(pos_list)\n",
    "    return tagged_sent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R30taRgf_WU6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('forward', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('serves', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('captain', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Portugal', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc = docs[0]\n",
    "tagged_sentences = ie_preprocess(first_doc)\n",
    "tagged_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYTwrZId_WU_"
   },
   "source": [
    "## Task 3\n",
    "Function that takes a list of tokens with POS tags for a sentence and returns a list of named entities (NE). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_named_entities(sent):\n",
    "    tree = nltk.ne_chunk(sent, binary=True)\n",
    "    named_entities = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NE':\n",
    "            entity = \"\"\n",
    "            for leaf in subtree.leaves():\n",
    "                entity += leaf[0] + \" \"\n",
    "            named_entities.append(entity.strip())\n",
    "    return named_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FijjdAPWFsp2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cristiano Ronaldo',\n",
       " 'Santos Aveiro',\n",
       " 'ComM',\n",
       " 'GOIH',\n",
       " 'Portuguese',\n",
       " 'Portuguese',\n",
       " 'Spanish',\n",
       " 'Real Madrid',\n",
       " 'Portugal']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences = ie_preprocess(docs[0])\n",
    "find_named_entities(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHMp7xtK_WVE"
   },
   "source": [
    "## Task 4\n",
    "\n",
    "Implement the `find_all_named_entities` function below to find **all** NEs in a given document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_named_entities(doc):\n",
    "    named_entities = []\n",
    "    named_list = []\n",
    "    tagged_sentences = ie_preprocess(doc)\n",
    "    for sent in tagged_sentences:\n",
    "        named_list.append(find_named_entities(sent))\n",
    "    for sublist in named_list:\n",
    "        for item in sublist:\n",
    "            named_entities.append(item)\n",
    "    return named_entities   # return a flat list and not a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_ajmnnOqJ8V6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities in 1st document:  56\n"
     ]
    }
   ],
   "source": [
    "NE_Doc1 = find_all_named_entities(docs[0])\n",
    "print(\"Named Entities in 1st document: \", len(NE_Doc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2AzD9MVNIx2"
   },
   "source": [
    "## Task 5\n",
    "\n",
    "Find named entities across **all** documents in `football_players.txt`, and save the result into a single flat list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_named_entities = []\n",
    "all_named_list=[]\n",
    "for doc in docs:\n",
    "    named_list = find_all_named_entities(doc)\n",
    "    all_named_list.append(named_list)\n",
    "for sublist in all_named_list:\n",
    "    for item in sublist:\n",
    "        all_named_entities.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jCNIrC_SNpHQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities in all documents:  380\n"
     ]
    }
   ],
   "source": [
    "print(\"Named Entities in all documents: \", len(all_named_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7-ma9lJ_WVJ"
   },
   "source": [
    "## Task 6\n",
    "\n",
    "Functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1:  Cristiano Ronaldo dos Santos Aveiro\n",
      "Doc 2:  Lionel Andrés \"Leo\" Messi \n",
      "Doc 3:  Neymar da Silva Santos Júnior \n",
      "Doc 4:  Ronaldo de Assis Moreira \n",
      "Doc 5:  Wayne Mark Rooney \n",
      "Doc 6:  Zlatan Ibrahimović \n",
      "Doc 7:  David Robert Joseph Beckham\n",
      "Doc 8:  Mesut Özil \n",
      "Doc 9:  Gareth Frank Bale \n",
      "Doc 10:  Andrés Iniesta Luján \n"
     ]
    }
   ],
   "source": [
    "def name_of_the_player(doc):\n",
    "    sent_list =[]\n",
    "    name_exp = re.compile(r'^([^,(])+')    #Extract words until '(' first occurrs \n",
    "    \n",
    "    for sent in doc:\n",
    "        sent_list += sent_tokenize(sent)\n",
    "    match = re.search(name_exp, sent_list[0])\n",
    "    if match:                            # to check if search is successful or not\n",
    "        name = match.group()\n",
    "    \n",
    "    return name\n",
    "\n",
    "print(\"Doc 1: \",name_of_the_player(docs[0]))\n",
    "print(\"Doc 2: \",name_of_the_player(docs[1]))\n",
    "print(\"Doc 3: \",name_of_the_player(docs[2]))\n",
    "print(\"Doc 4: \",name_of_the_player(docs[3]))\n",
    "print(\"Doc 5: \",name_of_the_player(docs[4]))\n",
    "print(\"Doc 6: \",name_of_the_player(docs[5]))\n",
    "print(\"Doc 7: \",name_of_the_player(docs[6]))\n",
    "print(\"Doc 8: \",name_of_the_player(docs[7]))\n",
    "print(\"Doc 9: \",name_of_the_player(docs[8]))\n",
    "print(\"Doc 10: \",name_of_the_player(docs[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1:  Portugal\n",
      "Doc 2:  Argentina\n",
      "Doc 3:  Brazil\n",
      "Doc 4:  Brazil\n",
      "Doc 5:  England\n",
      "Doc 6:  Sweden\n",
      "Doc 7:  England\n",
      "Doc 8:  German\n",
      "Doc 9:  Wales\n",
      "Doc 10:  Spain\n"
     ]
    }
   ],
   "source": [
    "def country_of_origin(doc):\n",
    "    NE_Doc = find_all_named_entities(doc)          #get all named entities in the doc passed to this function\n",
    "    sent_list=[]\n",
    "    pattern = \"the\\s(.*\\w.*)\\snational\\steam\"      #capture word between \"the\" and \"national team\"\n",
    "    \n",
    "    for sent in doc:\n",
    "        sent_list += sent_tokenize(sent)\n",
    "\n",
    "    for line in sent_list:\n",
    "        words = word_tokenize(line)\n",
    "        match = re.search(pattern,line)\n",
    "        if match:\n",
    "            country = match.group(1)\n",
    "            if country in NE_Doc:                # checking if the word matched with the pattern is a named entity\n",
    "                return country\n",
    "\n",
    "print(\"Doc 1: \",country_of_origin(docs[0]))\n",
    "print(\"Doc 2: \",country_of_origin(docs[1]))\n",
    "print(\"Doc 3: \",country_of_origin(docs[2]))\n",
    "print(\"Doc 4: \",country_of_origin(docs[3]))\n",
    "print(\"Doc 5: \",country_of_origin(docs[4]))\n",
    "print(\"Doc 6: \",country_of_origin(docs[5]))\n",
    "print(\"Doc 7: \",country_of_origin(docs[6]))\n",
    "print(\"Doc 8: \",country_of_origin(docs[7]))\n",
    "print(\"Doc 9: \",country_of_origin(docs[8]))\n",
    "print(\"Doc 10: \",country_of_origin(docs[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1:  5 February 1985\n",
      "Doc 2:  24 June 1987\n",
      "Doc 3:  5 February 1992\n",
      "Doc 4:  21 March 1980\n",
      "Doc 5:  24 October 1985\n",
      "Doc 6:  3 October 1981\n",
      "Doc 7:  2 May 1975\n",
      "Doc 8:  15 October 1988\n",
      "Doc 9:  16 July 1989\n",
      "Doc 10:  11 May 1984\n"
     ]
    }
   ],
   "source": [
    "def date_of_birth(doc):\n",
    "    pattern = \"born\\s(\\d.*\\w.*\\d)\\)\"           #capture digits, word and digits present after the word \"born\"\n",
    "    sent_list=[]\n",
    "    \n",
    "    for sent in doc:\n",
    "        sent_list += sent_tokenize(sent)\n",
    "    for line in sent_list:\n",
    "        words = word_tokenize(line)\n",
    "        match = re.search(pattern,line)\n",
    "        if match:\n",
    "            date = match.group(1) \n",
    "            return date\n",
    "print(\"Doc 1: \",date_of_birth(docs[0]))\n",
    "print(\"Doc 2: \",date_of_birth(docs[1]))\n",
    "print(\"Doc 3: \",date_of_birth(docs[2]))\n",
    "print(\"Doc 4: \",date_of_birth(docs[3]))\n",
    "print(\"Doc 5: \",date_of_birth(docs[4]))\n",
    "print(\"Doc 6: \",date_of_birth(docs[5]))\n",
    "print(\"Doc 7: \",date_of_birth(docs[6]))\n",
    "print(\"Doc 8: \",date_of_birth(docs[7]))\n",
    "print(\"Doc 9: \",date_of_birth(docs[8]))\n",
    "print(\"Doc 10: \",date_of_birth(docs[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_of_the_player(doc):\n",
    "    grammar =  r\"\"\"\n",
    "        NBAR:\n",
    "            {<NNP>+}  # Captures only nouns\n",
    "      \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    pattern = \"(club|plays|played\\sfor)\\s(.*\\w\\s\\w*|\\w\\snational\\wteam\\s\\w\\b)\"  #captues words after \"club\", \"plays\", \"played for\"\n",
    "    sent_list=[]\n",
    "    team_list = []\n",
    "    flag = 0\n",
    "    \n",
    "    for sent in doc:\n",
    "        line = sent_tokenize(sent)\n",
    "        for x in line:\n",
    "            match = re.search(pattern,x)\n",
    "            pos_list = []\n",
    "            if match:\n",
    "                team = match.group(2) \n",
    "                #print(\"LINE SELECTED: \",x)\n",
    "                #print(\"TEAM: \",team)\n",
    "                words = word_tokenize(team)\n",
    "                #print(\"WORDS: \",words)\n",
    "                pos_list = nltk.pos_tag(words)\n",
    "                #print(\"POS LIST: \",pos_list)\n",
    "                tree = chunker.parse(pos_list)          #check tagged sentence out of regex matched value with the grammer rule NBAR\n",
    "                if(flag == 0):                          #flag kept to check only 1st sentence which will have team name\n",
    "                    for subtree in tree.subtrees():\n",
    "                        nbar_phrase = \"\"\n",
    "                        if (subtree.label() == 'NBAR'):\n",
    "                            for leaf in subtree.leaves():\n",
    "                                nbar_phrase += leaf[0] + \" \"\n",
    "                            team_list.append(nbar_phrase.strip())\n",
    "                            flag = 1\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "        return team_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1:  ['Real Madrid', 'Portugal'] \n",
      "\n",
      "Doc 2:  ['FC Barcelona', 'Argentina'] \n",
      "\n",
      "Doc 3:  ['FC Barcelona', 'Brazil'] \n",
      "\n",
      "Doc 4:  ['FC Barcelona', 'September'] \n",
      "\n",
      "Doc 5:  ['Manchester United', 'England'] \n",
      "\n",
      "Doc 6:  ['Manchester United'] \n",
      "\n",
      "Doc 7:  ['Manchester United', 'Preston North End', 'Real Madrid', 'Milan', 'LA Galaxy', 'Paris Saint-Germain', 'England', 'Wayne Rooney'] \n",
      "\n",
      "Doc 8:  ['English', 'Arsenal'] \n",
      "\n",
      "Doc 9:  ['Real Madrid', 'Wales'] \n",
      "\n",
      "Doc 10:  ['FC Barcelona', 'Spain'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Doc 1: \",team_of_the_player(docs[0]),\"\\n\")\n",
    "print(\"Doc 2: \",team_of_the_player(docs[1]),\"\\n\")\n",
    "print(\"Doc 3: \",team_of_the_player(docs[2]),\"\\n\")\n",
    "print(\"Doc 4: \",team_of_the_player(docs[3]),\"\\n\")\n",
    "print(\"Doc 5: \",team_of_the_player(docs[4]),\"\\n\")\n",
    "print(\"Doc 6: \",team_of_the_player(docs[5]),\"\\n\")\n",
    "print(\"Doc 7: \",team_of_the_player(docs[6]),\"\\n\")\n",
    "print(\"Doc 8: \",team_of_the_player(docs[7]),\"\\n\")\n",
    "print(\"Doc 9: \",team_of_the_player(docs[8]),\"\\n\")\n",
    "print(\"Doc 10: \",team_of_the_player(docs[9]),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_of_the_player(doc):\n",
    "    grammar =  r\"\"\"\n",
    "        NBAR:\n",
    "            {<NN*>}                            # to find noun phrases\n",
    "        NP:\n",
    "            {<VBZ>?<IN>?<DT>*(<.*>)*<NBAR>*?}  # Above NBAR connected with verb, perposition or determinent\n",
    "\n",
    "      \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    pattern = \"(who\\splay|is\\s|as\\sa\\s)(.*?\\.)\"  #capture word after \"who play\" or \"is\" or \"as a\" until the first full stop\n",
    "    all_positions = [\"midfielder\",\"winger\",\"forward\",\"striker\",\"goalkeeper\",\"defender\"] #list of all positions in a football team\n",
    "    sent_list=[]\n",
    "    post_list=[]\n",
    "    for sent in doc:\n",
    "        line = sent_tokenize(sent)\n",
    "        for x in line:\n",
    "            match = re.search(pattern,x, re.MULTILINE)\n",
    "            pos_list = []\n",
    "            if match:\n",
    "                matched_pos = match.group(2) \n",
    "                if matched_pos:\n",
    "                    words = word_tokenize(matched_pos)\n",
    "                    pos_list = nltk.pos_tag(words)\n",
    "                    tree = chunker.parse(pos_list)\n",
    "                    for subtree in tree.subtrees():\n",
    "                        if subtree.label() == 'NP':  #check if the matched string follows the grammer rules defined in NP\n",
    "                            phrase = \"\"\n",
    "                            for leaf in subtree.leaves():\n",
    "                                phrase = phrase + leaf[0] + \" \"\n",
    "                                temp = phrase.strip()\n",
    "                                for x in all_positions:   #check if the word recevied from the after matching grammer is present in the defined set of positions \n",
    "                                    if x in temp:\n",
    "                                        post_list.append(x)\n",
    "                            \n",
    "                \n",
    "            else:\n",
    "                continue  #to continue to next line if regex pattern is not matched in given line\n",
    "        position = set(post_list) #removing duplicates from post_list\n",
    "        return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1:  {'forward'} \n",
      "\n",
      "Doc 2:  {'forward'} \n",
      "\n",
      "Doc 3:  {'forward'} \n",
      "\n",
      "Doc 4:  {'midfielder', 'forward'} \n",
      "\n",
      "Doc 5:  {'forward'} \n",
      "\n",
      "Doc 6:  {'striker'} \n",
      "\n",
      "Doc 7:  {'winger'} \n",
      "\n",
      "Doc 8:  {'midfielder'} \n",
      "\n",
      "Doc 9:  {'defender', 'winger'} \n",
      "\n",
      "Doc 10:  {'midfielder'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Doc 1: \",position_of_the_player(docs[0]),\"\\n\")\n",
    "print(\"Doc 2: \",position_of_the_player(docs[1]),\"\\n\")\n",
    "print(\"Doc 3: \",position_of_the_player(docs[2]),\"\\n\")\n",
    "print(\"Doc 4: \",position_of_the_player(docs[3]),\"\\n\")\n",
    "print(\"Doc 5: \",position_of_the_player(docs[4]),\"\\n\")\n",
    "print(\"Doc 6: \",position_of_the_player(docs[5]),\"\\n\")\n",
    "print(\"Doc 7: \",position_of_the_player(docs[6]),\"\\n\")\n",
    "print(\"Doc 8: \",position_of_the_player(docs[7]),\"\\n\")\n",
    "print(\"Doc 9: \",position_of_the_player(docs[8]),\"\\n\")\n",
    "print(\"Doc 10: \",position_of_the_player(docs[9]),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3VtWxBr_WVZ"
   },
   "source": [
    "## Task 6\n",
    "Identify and extract one other relation (besides team and player)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TR0GZrUB_WVa"
   },
   "outputs": [],
   "source": [
    "## This function is to extract various awards/titles won by players\n",
    "\n",
    "# NOTE: No awards/title has been mentioned for Player in Doc 8 \n",
    "\n",
    "def player_awards(doc):\n",
    "    grammar =  r\"\"\"\n",
    "        NBAR:\n",
    "            {<NNP>+<NN>}             #captures all words between noun phrases \n",
    "            {<DT><NNP>+}             #captures all words noun phrases after determinants \n",
    "            {<NNP>+<IN><DT><NN>}     #captures all words after noun phrases and ends with preposition, determinant, noun\n",
    "            {<JJ><NNP>+}             #captures all words starting with adjective and has one or more noun phrases \n",
    "      \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    pattern = \"(won|winning|win\\s)(.*\\w\\s|\\w\\sin)(.*?\\.)\"  #captures words after \"won\", \"win\", \"winning\" containing the word \"in\" until the 1st full stop occurrs.\n",
    "    NE_Doc = []\n",
    "    sent_list=[]\n",
    "    phrase_list = []\n",
    "    NE_Doc += find_all_named_entities(doc)  # call to store all named entities in given document\n",
    "    print()\n",
    "    for sent in doc:\n",
    "        line = sent_tokenize(sent)\n",
    "        for x in line:\n",
    "            match = re.search(pattern,x)\n",
    "            pos_list = []\n",
    "            if match:\n",
    "                award = match.group(2)\n",
    "                words = word_tokenize(award)\n",
    "                pos_list = nltk.pos_tag(words)  #creates POS list of all words matched by the regex pattern\n",
    "                tree = chunker.parse(pos_list)  #create tree of the POS list of words based on grammer rule\n",
    "                for subtree in tree.subtrees():\n",
    "                    if subtree.label() == 'NBAR': #check if the matched string follows the grammer rules defined in NBAR\n",
    "                        phrase = \"\"\n",
    "                        for leaf in subtree.leaves():\n",
    "                            phrase = phrase + leaf[0] + \" \"\n",
    "                        phrase_list.append(phrase.strip())\n",
    "                        \n",
    "                        \n",
    "            else:\n",
    "                continue\n",
    "        award_list = set(phrase_list)     #converting to set to remove duplicates\n",
    "\n",
    "        return award_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc 1:  {'a Club World', 'successive Premier League', \"FIFA Ballon d'Or\", 'UEFA Champions League title', 'FIFA World Player of the Year', 'European Golden Shoe', 'La Liga title', 'a FIFA Club World', \"Ballon d'Or\", 'the FA Cup'} \n",
      "\n",
      "\n",
      "Doc 2:  {\"FIFA Ballons d'Or\", 'the Golden Ball', 'Copas del', \"Ballon d'Or\", 'Year award', 'European Golden', 'Olympic gold'} \n",
      "\n",
      "\n",
      "Doc 3:  {'the Copa', 'the Golden Ball', 'successive Campeonato Paulista', 'the UEFA Champions League', 'a Copa'} \n",
      "\n",
      "\n",
      "Doc 4:  {'Rivaldo in an attacking', 'the FIFA World Player', 'the FIFA World Cup All-Star', 'the UEFA Champions League', \"Ballon d'Or\", 'Year award', 'alongside Ronaldo'} \n",
      "\n",
      "\n",
      "Doc 5:  {'the Premier League', 'the Premier League Goal', 'Day poll', 'Season award', 'the Football League Cup', 'the England Player', 'the Month', 'the FIFA Club World Cup', 'the UEFA Champions League', 'the Goal', 'the FA Cup', 'the Premier League Player', 'the FA Community Shield', 'the Year', 'the BBC'} \n",
      "\n",
      "\n",
      "Doc 6:  {'consecutive Ligue', 'another Scudetto'} \n",
      "\n",
      "\n",
      "Doc 7:  {'the UEFA Champions League', 'the United', 'Premier League title', 'La Liga championship', 'the FA Cup'} \n",
      "\n",
      "\n",
      "Doc 8:  set() \n",
      "\n",
      "\n",
      "Doc 9:  {'the FIFA Club World', 'the UEFA Squad', 'the UEFA Super Cup'} \n",
      "\n",
      "\n",
      "Doc 10:  {'Europe award', 'the Netherlands', 'the Man', 'the Match', 'the UEFA Best Player', 'the IFFHS World'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Doc 1: \",player_awards(docs[0]),\"\\n\")\n",
    "print(\"Doc 2: \",player_awards(docs[1]),\"\\n\")\n",
    "print(\"Doc 3: \",player_awards(docs[2]),\"\\n\")\n",
    "print(\"Doc 4: \",player_awards(docs[3]),\"\\n\")\n",
    "print(\"Doc 5: \",player_awards(docs[4]),\"\\n\")\n",
    "print(\"Doc 6: \",player_awards(docs[5]),\"\\n\")\n",
    "print(\"Doc 7: \",player_awards(docs[6]),\"\\n\")\n",
    "print(\"Doc 8: \",player_awards(docs[7]),\"\\n\")\n",
    "print(\"Doc 9: \",player_awards(docs[8]),\"\\n\")\n",
    "print(\"Doc 10: \",player_awards(docs[9]),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2.ipynb",
   "provenance": [
    {
     "file_id": "1EXXdimBbQY8nnqIs5hBXdG68r2hsk7HS",
     "timestamp": 1604940588321
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
